{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5109e1-8814-4812-ab86-cbce860db087",
   "metadata": {},
   "source": [
    "# 3.12.28 Pandas Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9df0d8-e451-4dcc-8196-b9a3a9a05cd9",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb3380b-00ab-4231-a6bd-6318d6e93f9a",
   "metadata": {},
   "source": [
    "Using the `GA Paid Search Traffic.csv` file from the `data` folder, let's load the \"third block of data\" from the file, that is, the Users' time series data, using the `pd.read_csv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779a921-4f91-4de0-ae91-a3c3fccd8cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b51e93-bdc5-4f5f-a61c-6afe05c99d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/GA\\ Paid\\ Search\\ Traffic.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf157087-7f70-4206-9880-12d3554f1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv(\"data/GA Paid Search Traffic.csv\", skiprows=18, nrows=31)\n",
    "print(ts.shape)\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e5f39-6bfd-47fe-b2e0-7bd9a34a3af3",
   "metadata": {},
   "source": [
    "There are no missing data in this time series, so let's **introduce NAs manually**. To do that, we will assign the `np.nan` object to some of the data points in the `Users` column. To make things more interesting, let's introduce a new method, `.sample()`, which **takes a sample from your DataFrame** (you can decide the size of the sample via the `n` parameter). We can use this sample to randomly select 5 rows of the DataFrame, we will then select the values in the `Users` column and replace them with the `np.nan` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478aee4-e6da-4d9e-b7d7-c206fac594db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.sample(n=5, random_state=42).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ada8ee-1b5d-4fd1-93fb-b5e016f9c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.iloc[ts.sample(n=5, random_state=42).index, 1] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1affba92-b593-4cef-88c0-fb5d0c610f71",
   "metadata": {},
   "source": [
    "First of all, let's **look at these missing values** we just created; in order to filter a DataFrame and show just the missing values, you can use the `.isnull()` method on the DataFrame / Series itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2cc1a6-057c-4749-9692-c7d2df7d5b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[ts.Users.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386a3b0-9752-4b68-9694-18cf7d532821",
   "metadata": {
    "tags": []
   },
   "source": [
    "When you're faced with **missing values** there are several paths you can choose to follow: \n",
    "\n",
    "- you can **drop the observations** containing missing values; this is probably the worst option, since you may be missing information, espectially if that row has values in other variables of the DataFrame\n",
    "- you can **fill them with zeros**; this solution may be useful and realistic in some situations (for instance if the value was missing because there were no sales on that day) and detrimental in others (if there was a technical issue with the recording of that data point, for example).\n",
    "- you can **fill them with the previous or with the next data point** (or with the mean of previous and next) if data is sequential in nature (like in our example); note that you can't apply this method if, for example, you're looking at city population and each row/observation is a different city\n",
    "- you can **fill them with the average** of that column / variable\n",
    "- you can do **some other kind of interpolation, prediction or filtering** via several methods such as [spline interpolation](https://en.wikipedia.org/wiki/Spline_interpolation), [moving average](https://en.wikipedia.org/wiki/Moving_average) methods and, in more complex scenarios, [linear regression](https://en.wikipedia.org/wiki/Linear_regression). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2721db1-cefb-4fcf-b993-97c3699cb0b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's see how we can fill these missing values using the `.fillna()` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b8db6-9998-4128-b9eb-793fb0c8dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with zeros\n",
    "ts_fill = ts.copy()\n",
    "ts_fill.fillna(0).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a65876-7594-49d9-8b7a-6e96a72fcecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with next valid observation \n",
    "ts_fill.fillna(method='bfill').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f10f3-fe5f-49ed-b8df-11baf593ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with last valid observation\n",
    "ts_fill.fillna(method='ffill').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa759fed-67f6-4475-8a91-480f269174e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with average of column\n",
    "ts_fill.fillna(ts_fill.Users.mean()).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d902f95-d0de-45ec-b1d7-8f95ee762d00",
   "metadata": {},
   "source": [
    "Using the `.interpolate()` method, you can also fill all the missing values using a linear interpolation. Check out the [method parameter](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html#pandas.DataFrame.interpolate) for a list of all the options available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223da20-a7b9-4d08-8228-37a0ad256bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with interpolation --> see the method parameter for more options!\n",
    "ts_fill.interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f8234e-fc4b-4e43-b70e-dae6792c499b",
   "metadata": {},
   "source": [
    "### Joining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b58e32-baa1-4d11-b92d-fa84bccc8b6b",
   "metadata": {},
   "source": [
    "You can connect rows in DataFrames based on one or more keys via the `pd.merge()` [function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html). These kind of operations are analogous to **join operations** performed on a relational database using the SQL language. \n",
    "\n",
    "We will be using the European Soccer Database, a collection of CSV files containing matches data and metadata about the teams and leagues they play in: \n",
    "\n",
    "- match.csv\n",
    "- team.csv\n",
    "- leagues.csv\n",
    "\n",
    "Let's start by loading and looking into the first two datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f822bb-6d25-4041-97f2-c39162b6f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_raw = pd.read_csv(\"data/European Soccer Database/match.csv\")\n",
    "match_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fb28b-6a74-40b5-b83a-47b74bdd6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "team = pd.read_csv(\"data/European Soccer Database/team.csv\")\n",
    "team.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350758d5-72cb-4808-a29d-4c8f40ba1a96",
   "metadata": {},
   "source": [
    "By looking at the data, we can see that there are the following **entity relationships** that link each table to one another: \n",
    "\n",
    "<img src=\"img/European Soccer DB - ERD.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abd134-73a9-4dc0-8ef5-51108c89a4ab",
   "metadata": {},
   "source": [
    "For example, let's start by [merging](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) the `match` with the `team` dataset via the key-pairs `home_team_api_id <-> team_api_id` and then on `away_team_api_id <-> team_api_id`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214fadde-e08a-4eaa-a402-880db322c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on home team\n",
    "match = match_raw.merge(team[['team_api_id','team_long_name']], how='left', left_on='home_team_api_id', right_on='team_api_id')\n",
    "match.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5e189-840a-4d61-9e67-3e260c520798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on away team\n",
    "match = match.merge(team[['team_api_id','team_long_name']], how='left', left_on='away_team_api_id', right_on='team_api_id')\n",
    "match.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c71525-54cd-47e3-8bae-764e81283d66",
   "metadata": {},
   "source": [
    "The whole thing looks a bit messy, let's clean it up a bit by keeping just the relevant columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716811f-16fc-4f23-8225-094a4412b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "match.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74e0e2-6fcc-46e2-867e-14ac5b1b9d06",
   "metadata": {},
   "source": [
    "I **reorder some of the columns** using the double-bracket `[[ ]]` operator; note that if I omit some of the column names, they will be automatically dropped from the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2edad8-0fc5-4325-b162-cda73fa97ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns in the DataFrame (omitting a column name will implicitly drop that column)\n",
    "match = match[['match_api_id', 'league_id', 'season', 'stage', 'date', \n",
    "               'home_team_api_id', 'away_team_api_id', 'team_long_name_x', 'team_long_name_y', \n",
    "               'home_team_goal', 'away_team_goal']]\n",
    "match.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3f4bd-1d32-44a7-933d-305dcde5203e",
   "metadata": {},
   "source": [
    "Then I **rename some of the columns** in order to make them easier to read and interpret. Check out the [documentation page](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) of the `.rename()` method for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a05b2d-abcf-4f27-8da5-a7fe497e750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns in the DataFrame\n",
    "match = match.rename(columns={\"team_long_name_x\": \"home_team_name\", \"team_long_name_y\": \"away_team_name\"})\n",
    "match.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6ff1b-452d-4b37-8e34-48810a8a664d",
   "metadata": {},
   "source": [
    "### Grouping data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31260475-7e28-48d9-9dbb-b4e291be0efb",
   "metadata": {},
   "source": [
    "After all this work we have a nicely shaped dataset, so it would be interesting to **investigate it further by grouping and aggregating its variables**. To do this, we can use the `.groupby()` [method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) to combine all the elements of the same category and a function like `.size()`, `.sum()` or `.mean()` to aggregate the results over a specific metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e3171-d099-4668-b0ea-02085e3f1913",
   "metadata": {},
   "source": [
    "Let's say we're interested in finding out the sum and the average home goals scored in the whole dataset; we can use the `.agg()` [method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc18b44-1ef7-4137-a9af-0b5f734aabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "match['home_team_goal'].agg(['min', 'mean', 'sum', 'max', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632d5cb-7afd-4e58-a92f-e919e064667f",
   "metadata": {},
   "source": [
    "If we want to aggregate these two metrics over each `season` group, we can combine the **group by** with the aggregation method we just introduced: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085a922-d5e7-409f-9576-24a031176059",
   "metadata": {},
   "outputs": [],
   "source": [
    "match.groupby('season')['home_team_goal'].agg(['min', 'mean', 'sum', 'max', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eecf95d-334d-42f4-a678-e9f61c18ff27",
   "metadata": {},
   "source": [
    "Check out [this webpage](https://pbpython.com/groupby-agg.html) for more examples on the use of groupby() and agg(). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b95e8-e7a7-46ac-bdb2-d8dce394c4fb",
   "metadata": {},
   "source": [
    "If we're interested in the number of matches played in each season (remember that in this dataset, each row represents a different match), we can use the `.size()` method, which **returns the total number of row count for each group** *(note that you don't need to specify a column, you can use it over the whole DataFrame)*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e4369-0097-4518-99fc-d9cd30821633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size is the equivalent of count\n",
    "match.groupby('season').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e0541-7bc8-43c1-8ad8-46c7c50b785b",
   "metadata": {},
   "source": [
    "If, instead, we wanted to find out the **number of matches played in each month** of every `season`, we would need to create a new `month` variable (where we [extract the month from the date](https://www.interviewqs.com/ddi-code-snippets/extract-month-year-pandas) column) and include it in the group by statement: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491ff20a-d9b4-4a2e-a475-91576ecbc834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the month from the date column and save it as a new variable\n",
    "match['month'] = pd.DatetimeIndex(match['date']).month\n",
    "match.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f98353-eef5-4d0f-baf3-7b853e8a2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = match.groupby(['season', 'month']).size()\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90364c33-1bd7-4d3d-b65c-e69821a61dcb",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee28505-5c67-4aec-9c1a-75744c331dbc",
   "metadata": {},
   "source": [
    "Great, this is what we wanted! However, you may have noticed that **it returned a multi-index DataFrame**; I personally prefer working with single-index DataFrames, so if you include the `as_index=False` parameter in the `.groupby()` method, you can flatten the multi-index to a single-index DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09a344-836a-477b-a65e-8d513d5f0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = match.groupby(['season', 'month'], as_index=False).size()\n",
    "df2.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c5dff7-ee9f-4be6-81cc-4fc68a61eb31",
   "metadata": {},
   "source": [
    "Now, let's say we wanted to **find out the total number of goals** (home + away) per team in the most recent available season; we'd need to split the problem into separate subproblems: \n",
    "\n",
    "1. first we filter the data to show just the most recent season\n",
    "2. then we group all the home teams and calculate the total home goals\n",
    "3. then we group all the away teams and calculate the total away goals\n",
    "4. finally, we join the two DataFrames and calculate the total number of goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4961170-4762-40fe-88bb-34419dd7fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out which season is the most recent one\n",
    "match.season.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a84765e-a7d9-4658-a0af-5a6f7b8878a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "match15_16 = match.loc[match['season']=='2015/2016'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf24de-7116-490b-85b9-802499fa9588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame that contains all the home team goals\n",
    "home = match15_16.groupby(['home_team_name'], as_index=False)['home_team_goal'].sum()\n",
    "home.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a474ec-ed09-4e91-9f1d-3db9416f73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame that contains all the away team goals\n",
    "away = match15_16.groupby(['away_team_name'], as_index=False)['away_team_goal'].sum()\n",
    "away.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fccb915-c15e-4bce-a1f1-c443e689f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame that contains all the home + away team goals\n",
    "all_goals = home.merge(away, how='inner', left_on='home_team_name', right_on='away_team_name')\n",
    "all_goals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb45664-d600-46e3-852a-117366d4fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new variable containing the total number of goals (home + away)\n",
    "all_goals['total_goal'] = all_goals['home_team_goal'] + all_goals['away_team_goal']\n",
    "all_goals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1e954-c74d-4e76-9394-f2d729b07d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename variable to team_name\n",
    "all_goals = all_goals.rename(columns={\"home_team_name\":\"team_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385397da-d0f6-4bf5-bafb-e46008097281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop away_team_name variable\n",
    "all_goals.drop('away_team_name', axis=1, inplace=True)\n",
    "all_goals.sort_values('total_goal', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd137a-5e5e-4c73-9e24-a5d8244d5762",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619dee19-ddc9-4a2b-9e43-481c1e99d3ff",
   "metadata": {},
   "source": [
    "Perform the following tasks and /or answer to the following questions: \n",
    "\n",
    "1. Load the `leagues.csv` dataset and call it `leagues`\n",
    "2. Create a `.copy()` of the `match` dataset and call it `match_teams`\n",
    "3. Join the `match_teams` and the `leagues` dataset with a left join using their common keys\n",
    "4. Find out the number of matches played by each league `name` in each `season`\n",
    "5. Which combination of season-league has the lowest number of matches played?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f0760-6e43-4e20-a374-f266dfc9019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f64bd66-eb06-4d37-ace1-8c642a389a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed4c36-9a4c-444b-8f86-b954892e23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b5140-700f-4e0e-ac7b-a71a2d889612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec50ab9-9fb6-4b40-ba97-2de21a309eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
